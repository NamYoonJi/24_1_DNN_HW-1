{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Task 1"
      ],
      "metadata": {
        "id": "tc65guX5FFF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "AOaM8j21uid8"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Neural nets with pytorch"
      ],
      "metadata": {
        "id": "dpzs0yuiAg2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensors for inputs x and weights w1, w2.\n",
        "x_torch = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]) #concat x1, x2 into one matrix\n",
        "w1_torch = torch.tensor([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8], [0.9, 1.0, 1.1, 1.2]], requires_grad=True)\n",
        "w2_torch = torch.tensor([[0.2, 0.3], [0.4, 0.5], [0.6, 0.7], [0.8, 0.9]], requires_grad=True)\n",
        "\n",
        "# Forward pass\n",
        "z1_torch = torch.matmul(x_torch, w1_torch)\n",
        "a1_torch = F.relu(z1_torch)\n",
        "z2_torch = torch.matmul(a1_torch, w2_torch)\n",
        "output_torch = F.softmax(z2_torch, dim=1)\n",
        "\n",
        "print(\"PyTorch Neural Network Output:\")\n",
        "print(\"output for x1:\", output_torch[0])\n",
        "print(\"output for x2:\", output_torch[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LE0kW03CK41u",
        "outputId": "33b9ba21-8758-448d-a6be-6da99a1ce2b4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Neural Network Output:\n",
            "output for x1: tensor([0.1324, 0.8676], grad_fn=<SelectBackward0>)\n",
            "output for x2: tensor([0.0145, 0.9855], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------"
      ],
      "metadata": {
        "id": "mpFC_NfrM7uJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural net with numpy"
      ],
      "metadata": {
        "id": "8JzDuWbnMtpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Activation functions\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))\n",
        "    return exp_x / exp_x.sum(axis=0)\n",
        "\n",
        "# Given weights and inputs\n",
        "x = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "w1 = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8], [0.9, 1.0, 1.1, 1.2]])\n",
        "w2 = np.array([[0.2, 0.3], [0.4, 0.5], [0.6, 0.7], [0.8, 0.9]])\n",
        "\n",
        "# Forward pass\n",
        "z1 = x.dot(w1) # Input to hidden\n",
        "a1 = relu(z1) # Hidden activation\n",
        "z2 = a1.dot(w2) # Hidden to output\n",
        "output = softmax(z2.T).T # Output activation\n",
        "\n",
        "print(\"Numpy Neural Network Output:\")\n",
        "print(\"output for x1:\", output[0])\n",
        "print(\"output for x2:\", output[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_3Di9uTMyW_",
        "outputId": "419d4f31-cbf0-4d7c-c63f-540c583b61fb"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numpy Neural Network Output:\n",
            "output for x1: [0.13238887 0.86761113]\n",
            "output for x2: [0.01448572 0.98551428]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------"
      ],
      "metadata": {
        "id": "ilCziJvyM2g2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 2\n",
        " - gradient"
      ],
      "metadata": {
        "id": "LXWwfcXjFPZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pytorch"
      ],
      "metadata": {
        "id": "s2PMa3S3pvR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# making forward process into function for pytorch\n",
        "def forward(x):\n",
        "    z1_torch = torch.matmul(x_torch, w1_torch) # input x into first weight layer w1\n",
        "    a1_torch = F.relu(z1_torch) # activation with relu\n",
        "    z2_torch = torch.matmul(a1_torch, w2_torch) # input a1 into second weight layer w2\n",
        "    output_torch = F.softmax(z2_torch, dim=1) # activation with softmax\n",
        "    return output_torch\n",
        "\n",
        "# definition of cross entropy loss\n",
        "def cross_entropy_loss(x, y):\n",
        "    delta = 1e-7\n",
        "    return -torch.sum(y*torch.log(x+delta))"
      ],
      "metadata": {
        "id": "SobshM_Xz3N-"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# given data\n",
        "x_torch = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "w1_torch = torch.tensor([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8], [0.9, 1.0, 1.1, 1.2]], requires_grad=True)\n",
        "w2_torch = torch.tensor([[0.2, 0.3], [0.4, 0.5], [0.6, 0.7], [0.8, 0.9]], requires_grad=True)\n",
        "y_torch = torch.tensor([[0,1],[1,0]]) #target label y.\n",
        "\n",
        "\n",
        "# just one epoch for calculating gradient.\n",
        "n_iter = 1\n",
        "\n",
        "for i in range(n_iter):\n",
        "    Y_pred = forward(x_torch)\n",
        "    loss = cross_entropy_loss(Y_pred, y_torch)\n",
        "    loss.backward() # backpropagation in order to get gradient\n",
        "\n",
        "print(w1_torch.grad)\n",
        "# print(w2_torch.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQ-cOTkO0iFC",
        "outputId": "f348f2fd-4b23-414b-d7fa-ceab2c4b9102"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3810, 0.3810, 0.3810, 0.3810],\n",
            "        [0.4663, 0.4663, 0.4663, 0.4663],\n",
            "        [0.5516, 0.5516, 0.5516, 0.5516]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------------------------"
      ],
      "metadata": {
        "id": "VyBE0jYhzzMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient with numpy"
      ],
      "metadata": {
        "id": "6sbrEVrRNay0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Cross Entropy Loss for numpy (using numpy only)\n",
        "def CrossEntropy_np(y_pred, y):\n",
        "    return -1 * np.sum(y * np.log(y_pred))"
      ],
      "metadata": {
        "id": "eayH7WStEm5Y"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a class for convenience\n",
        "class Neural_Net_np:\n",
        "    def __init__(self):\n",
        "        #weights\n",
        "        self.w1 = np.array([[0.1, 0.2, 0.3, 0.4],\n",
        "               [0.5, 0.6, 0.7, 0.8],\n",
        "               [0.9, 1.0, 1.1, 1.2]])\n",
        "        self.w2 = np.array([[0.2, 0.3],\n",
        "               [0.4, 0.5],\n",
        "               [0.6, 0.7],\n",
        "               [0.8, 0.9]])\n",
        "\n",
        "    def ReLU(self, z):\n",
        "        return np.maximum(0, z)\n",
        "\n",
        "    # Added relu derivative for backward pass\n",
        "    def ReLU_derivative(self, x):\n",
        "        return (x > 0).astype(float)\n",
        "\n",
        "    def softmax(self, z):\n",
        "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.z1 = x.dot(self.w1)\n",
        "        self.a1 = self.ReLU(self.z1)\n",
        "        self.z2 = self.a1.dot(self.w2)\n",
        "        self.a2 = self.softmax(self.z2)\n",
        "        return self.a2\n",
        "\n",
        "    def cross_entropy_loss(self, y_pred, y):\n",
        "        return -1 * np.sum(y * np.log(y_pred))\n",
        "\n",
        "    def backward(self, x, y):\n",
        "        # Gradient of the loss\n",
        "        # Derive using chain rule\n",
        "        dL_dz2 = self.a2 - y\n",
        "        dL_da1 = dL_dz2.dot(self.w2.T)\n",
        "        dL_dz1 = dL_da1 * self.ReLU_derivative(self.z1)\n",
        "        dL_dw1 = x.T.dot(dL_dz1)\n",
        "        return dL_dw1"
      ],
      "metadata": {
        "id": "X5rORWEbW7FU"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NUMPY\n",
        "model = Neural_Net_np() #initialize model\n",
        "x = np.array([[1.0,2.0,3.0],[4.0,5.0,6.0]])\n",
        "output = model.forward(x) # pass x through neural network\n",
        "y = np.array([[0,1], [1,0]])\n",
        "loss = model.cross_entropy_loss(output, y) # get the loss using cross entropy loss\n",
        "grad_w1 = model.backward(x, y)\n",
        "\n",
        "\n",
        "print(\"Gradient of Loss with respect to w1:\\n\" , grad_w1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKEhKkjsgo8n",
        "outputId": "1c25e02b-2630-4c7d-9fa3-8804d11dcb49"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient of Loss with respect to w1:\n",
            " [[0.38096682 0.38096682 0.38096682 0.38096682]\n",
            " [0.46627936 0.46627936 0.46627936 0.46627936]\n",
            " [0.5515919  0.5515919  0.5515919  0.5515919 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------"
      ],
      "metadata": {
        "id": "Twon8hx3qLDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3"
      ],
      "metadata": {
        "id": "KANQMRByKFEg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ↓dropout with pytorch"
      ],
      "metadata": {
        "id": "NBZQveSZv3P1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Set learning rate as 0.01\n",
        "learning_rate = 0.01\n",
        "\n",
        "#inputs and weights\n",
        "x_torch = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "w1_torch = torch.tensor([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8], [0.9, 1.0, 1.1, 1.2]], requires_grad=True)\n",
        "w2_torch = torch.tensor([[0.2, 0.3], [0.4, 0.5], [0.6, 0.7], [0.8, 0.9]], requires_grad=True)\n",
        "y_torch = torch.tensor([[0,1],[1,0]])\n",
        "\n",
        "# update through 100 epochs\n",
        "for epoch in range(100):\n",
        "    # forward pass\n",
        "    z1_torch = torch.matmul(x_torch, w1_torch)\n",
        "    a1_torch = F.dropout(F.relu(z1_torch), p=0.4, training=True) #relu in inside the dropout\n",
        "    z2_torch = torch.matmul(a1_torch, w2_torch)\n",
        "    output = F.softmax(z2_torch)\n",
        "\n",
        "    # Gain loss\n",
        "    loss = cross_entropy_loss(output, y_torch)\n",
        "\n",
        "    # Backpropagate\n",
        "    loss.backward()\n",
        "\n",
        "    # update weights\n",
        "    with torch.no_grad():   # set torch.no_grad since we don't train at update step\n",
        "        w1_torch -= learning_rate * w1_torch.grad\n",
        "        w2_torch -= learning_rate * w2_torch.grad\n",
        "\n",
        "        # gradient initialization\n",
        "        w1_torch.grad.zero_()\n",
        "        w2_torch.grad.zero_()\n",
        "\n",
        "print(w1_torch)\n",
        "print(w2_torch)"
      ],
      "metadata": {
        "id": "gBnJC2WJiMvx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29c81a42-48c0-49d3-b24b-00db7ba2101f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0133, 0.0818, 0.1945, 0.2887],\n",
            "        [0.3564, 0.4189, 0.5409, 0.5930],\n",
            "        [0.6995, 0.7559, 0.8872, 0.8973]], requires_grad=True)\n",
            "tensor([[0.2899, 0.2101],\n",
            "        [0.4139, 0.4861],\n",
            "        [0.5957, 0.7043],\n",
            "        [0.8643, 0.8357]], requires_grad=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-724395e4756c>:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = F.softmax(z2_torch)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ↓dropout with Numpy"
      ],
      "metadata": {
        "id": "PW2PwwiXtEN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class update\n",
        "class Neural_Net_np:\n",
        "    def __init__(self):\n",
        "        self.w1 = np.array([[0.1, 0.2, 0.3, 0.4],\n",
        "               [0.5, 0.6, 0.7, 0.8],\n",
        "               [0.9, 1.0, 1.1, 1.2]])\n",
        "        self.w2 = np.array([[0.2, 0.3],\n",
        "               [0.4, 0.5],\n",
        "               [0.6, 0.7],\n",
        "               [0.8, 0.9]])\n",
        "\n",
        "    def ReLU(self, z):\n",
        "        return np.maximum(0, z)\n",
        "\n",
        "    def ReLU_derivative(self, x):\n",
        "        return (x > 0).astype(float)\n",
        "\n",
        "    def softmax(self, z):\n",
        "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.z1 = x.dot(self.w1)\n",
        "        self.a1 = self.ReLU(self.z1)\n",
        "        self.z2 = self.a1.dot(self.w2)\n",
        "        self.a2 = self.softmax(self.z2)\n",
        "        return self.a2\n",
        "\n",
        "    def cross_entropy_loss(self, y_pred, y):\n",
        "        return -1 * np.sum(y * np.log(y_pred))\n",
        "\n",
        "\n",
        "    def backward(self, x, y):\n",
        "        # Gradient of the loss with respect to softmax input\n",
        "        self.dL_dz2 = self.output - y\n",
        "        self.dL_dw2 = self.a1.T.dot(self.dL_dz2)\n",
        "        self.dL_da1 = self.dL_dz2.dot(self.w2.T)\n",
        "        self.dL_dz1 = self.dL_da1 * self.ReLU_derivative(self.z1)\n",
        "        self.dL_dw1 = x.T.dot(self.dL_dz1)\n",
        "        return self.dL_dw1, self.dL_dw2\n",
        "        # return both dL_dw1 and dL_dw2 for weight update\n",
        "\n",
        "    # Added Dropout function\n",
        "    def dropout(self, a1, rate=0.4):\n",
        "        # Generate a mask to drop out neurons\n",
        "        mask = np.random.binomial(1, 1-rate, size = a1.shape)\n",
        "        return a1 * mask\n",
        "\n",
        "    # Added dropout layer after the first activation (ReLU)\n",
        "    def forward_with_dropout(self, x):\n",
        "        self.z1 = x.dot(self.w1)\n",
        "        self.a1 = self.ReLU(self.z1)\n",
        "        self.a1_dropout = self.dropout(self.a1)\n",
        "        self.z2 = self.a1_dropout.dot(self.w2)\n",
        "        self.output = self.softmax(self.z2)\n",
        "        return self.output\n",
        "\n",
        "    # Added update weight function\n",
        "    def update_weight(self, grad_w1, grad_w2, lr=0.01):\n",
        "        # get the gradient with respect to w1 and w2, then multiply with learning rate(0.01)\n",
        "        self.w1 -= lr * grad_w1\n",
        "        self.w2 -= lr * grad_w2"
      ],
      "metadata": {
        "id": "p0lLCM8LQt7_"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Neural_Net_np()\n",
        "\n",
        "#inputs and y values\n",
        "x = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "y = np.array([[0, 1], [1, 0]])\n",
        "\n",
        "for epoch in range(100):\n",
        "    # forward pass\n",
        "    output = model.forward_with_dropout(x)\n",
        "    # gain loss\n",
        "    loss = model.cross_entropy_loss(output, y)\n",
        "    # backpropagation\n",
        "    grad_w1, grad_w2 = model.backward(x, y)\n",
        "    # update weight\n",
        "    model.update_weight(grad_w1, grad_w2, lr = 0.01)\n",
        "\n",
        "print(model.w1)\n",
        "print(model.w2)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6wRScYqvCwE",
        "outputId": "20f47a20-8f89-4232-d2c6-e8adb6a7d3ba"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.06099949 0.18883286 0.31666624 0.44449962]\n",
            " [0.47403567 0.5789945  0.68395333 0.78891215]\n",
            " [0.88707185 0.96915613 1.05124041 1.13332469]]\n",
            "[[0.17448702 0.32551298]\n",
            " [0.42927113 0.47072887]\n",
            " [0.68405524 0.61594476]\n",
            " [0.93883934 0.76116066]]\n"
          ]
        }
      ]
    }
  ]
}